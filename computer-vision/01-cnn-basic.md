# 합성곱

- 합성곱이란
	- 입력 위에서 필터를 슬라이딩 하면서 진행
	- 각 위치에서 겹치는 값 곱하고 합산
	- 전통적인 신호처리와 다르게 훈련중 필터가 학습됨
	- CNN 에서는 기존 신호처와 다르게 필터를 플립하지 않음

- 역할
	- 특징 추출
	- 패턴을 강조하는 특징맵 생성

- 스트라이드
	- 필터가 이동하는 스텝 수, 기본값: 1

- 패딩
	- 입력 이미지 주변에 추가하는 여분의 픽셀 (보통은 0을 추가)
	- 가장자리에서 정보 손실을 방지하기 위함
	- same padding: 합성곱층 출력 크기를 입력 크기와 동일하게

- 함성곱 층에서 출력 크기 계산 방법
	- $W_{out} = \frac{W_{in} - K + 2P}{S} + 1$
	- $H_{out} = \frac{H_{in} - K + 2P}{S} + 1$
	- $W_{in}, H_{in}$: 입력 너비와 높이
	- $K$: 커널(필터) 크기
	- $P$: 패딩
	- $S$: Stride

- CNN 에서 필터를 플립하지 않는 이유
	- 기존 신호처리는 패턴이 미리 정의되어 있어 컨볼루션시 필터를 뒤집어 비교함
	- 플립하는 이유는 두 함수의 정렬을 올바르게 맞추어 유사도를 제대로 비교하기위함
	- CNN 은 필터를 플립마저 학습하므로 굳이 뒤집을 필요가 없음




# 풀링

- 목적
	- 공간 크기 축소 ->  계산 비용 낮춤
	- 다음 합성곱 전 중요 정보를 유지 및 강조
	- 이동 불변성 도입
	- 활성화 함수를 기반으로 비선형성 제공

> ###### 이동 불변성이란? (shift invariance)
> 
> - 특징이 입력 이미지에서 약간 이동하더라도 여전히 인식되는 것 
> - 이미지가 이동해도 커널안에서 동일한 최대 값을 가지기 때문
> - 무엇에 집중 어디에는 덜 집중
> - 좁은 범위에만 적용됨


- 종류
	- 평균 폴링
		- 각 윈도우의 평균 값을 사용
		- 특징 맵을 부드럽게 만듦
		- 미세한 변화나 노이즈에 강하고 정보 손실/왜곡을 최소화
	- 최대 폴링
		- 각 윈도우의 최대값을 사용
		- 강한 활성을 보존하여 주요 특징을 강조

# GAP 풀링

- FC 층 대안으로 사용 가능
- GAP 는 각 특징 맵의 평균값을 계산함
-  $H \times W$ 크기의 특징 맵을 채널당 단일 값으로 변환함
- 출력은 $1 \times 1 \times C$ 텐서가 됨 (C = 채널 수)
- 각 채널에 대해 다음과 같이 계산: $GAP(X_c) = \frac{1}{H \times W}\sum_{i=1}^{H}\sum_{j=1}^{W} X_c(i,j)$

- 동기
	- FC 계층의 한계
	- 파라미터가 너무 많음 -> 높은 계산 비용, 과적합 위험험
	- 고정된 입력 크기 -> 유연성 제안

- GAP 장점
	- 파라미터 수를 0 으로 줄임
	- 모델을 더 가볍고 유연하게 만듦
	- 여전히 엔드 투 엔드 학습 허용


# 비선형성의 중요성

- 중요성
	- 비선형성이 없으면 신경망은 단지 하나의 선형 함수가 됨
	- 여러 선형 계층을 쌓아도 여전히 단일 선형 계층처럼 동작함
	- 복잡한 패턴을 모델링 하려면 비선형 함수가 필요함

- 풀링 후 활성화 함수
	- CNN 에서 풀링 계층은 일반적으로 활성화 함수가 뒤따름
	- 활성화 함수는 모델에 비선형성을 도입함

- 선형성의 조건
	- 1. 가산성 – $f(x + y) = f(x) + f(y)$
	- 2. 동차성(스케일링) – $f(a \cdot x) = a \cdot f(x)$

- 활성화 함수
	- 종류: ReLU $f(x) = \max(0, x)$, Sigmoid $f(x) = \frac{1}{1+e^{-z}}$, tanh $f(x) = \tanh(x)$
	- 이러한 함수는 $f(a \cdot x) \neq a \cdot f(x)$ 이 때문에 선형성을 깨뜨림
	- 활성화 함수는 선형성을 깨뜨려 네트워크가 복잡한 특징을 학습하게 함


