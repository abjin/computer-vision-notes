# LeNet-5 이란 무엇인가

- 얄 르쿤이 1988년에 개발함
- 최초의 합성곱 신경망임
- 손글씨 인식을 위해 개발함
- 현대 딥러닝 모델의 기반을 마련함

# LeNet-5 의 역사적 중요성

- 실제 사례에 적용
	- 우편번호 및 은행 수표와 같은 실제 숫자인식 작업을 위해 개발
	- 실제 산업의 문제를 해결
- 자동 특징 추출
	- 기존에 수동으로 진행했던 특징 추출 엔지니어링을 end-to-end 파이프라인으로 대체
	- 모델이 원시 데이터로부터 윤곽선, 형태, 질감 표현을 직접 학습할 수 있게함
- 효율적인 설계
	- 가중치 공유, 지역 수용 영역 그리고 서브샘플링을 사용해서 파라미터 수를 크게 줄임
	- 파라미터 수를 줄임으로서 적은 계산 자원과 데이터로 높은 일반화 성능을 보여줌
- 일반화를 위한 설계
	- 훈련 데이터가 제한적일때 모델 복잡도를 낮추는 것의 중요성을 보여줌
	- 일반화 격차: $E_{test} - E_{train} = k \left(\frac{h}{P}\right)^\alpha$ ; $k, \alpha$: 상수 (경험적으로 결정됨)
	- 일반화 격차는 모델 복잡도 $h$ 에 따라 증가, 데이터 수 $p$가 많을 수록 감소
	- 공유 가중치, 지역 연결, 서브샘플링을 사용해 모델 복잡도를 줄이도록 설계


# LeNet-5 아키텍처 완전 분석
## LeNet-5 전체 구조 개요

> 입력층 -> 3개의 합성곱층 2개의 풀링층 번갈아서 -> FC 계층

- c3채널 6개 -> 16개
- 5곱5 conv, 2곱2 풀링

- 입력: 32 x 32
- 합성곱: 특징 추출
- 풀링층: 특징 맵 크기 줄임 -> 빠르고 간단
- FC: 모든 특징맵 결합 최종판단
- 출력: 최종 예측 점수 제공


## C1 - 첫 번째 합성곱 계층

- 입력: 32×32 그레이스케일 이미지
- 6개의 5×5 크기 필터 → 크기 28×28의 6개 특징 맵 생성 (즉, 6@28×28)
- 패딩 없음, stride = 1
- 활성화 함수: tanh
- 파라미터 수: $(5 \times 5 \times 1 + 1) \times 6 = 156 \text{ 파라미터}$


## S2 - 서브 샘플링 계층 (평균 폴링)


- 입력: 크기 28 x 28 의 6개 특징 맵
- 풀링 연산: 2x2 평균 풀링
- 스트라이드: 2 → 각 맵을 14×14로 축소
- 활성화 함수: 풀링 후 tanh
- 훈련해야할 파라미터 개수: (가중치 + 바이어스) * 특성맵개수 = (1+1)*6 = 12


## C3 - 두 번째 합성곱 계층

- 입력: S2 계층의 6개 특징 맵 (크기 14 x 14)
- 필터: 5x5 크기
- 출력: 16개의 특징 맵
- stride: 1
- padding: 없음
- **특징: 모든 입력 맵이 모든 출력 맵에 연결 되지 않음 :그룹화된 연결**



## S4 – 두 번째 서브샘플링(평균 풀링) 계층

- 입력: C3의 크기 10×10인 16개 특징 맵
- 풀링 방법: 평균 풀링
- 필터 크기: 2×2, Stride: 2
- 출력: 크기 5×5인 16개 특징 맵
- 활성화 함수: tanh
- 훈련해야할 파라미터 개수: (가중치 + 바이어스)*특성맵개수 = (1 + 1)*16 = 32


## C5 – 세 번째 합성곱 계층

- 입력: S4의 크기 5×5인 16개 특징 맵
- 출력: 크기 1×1인 120개 특징 맵
- 어떻게: 각 필터는 모든 16개 입력 맵을 커버
- 필터 크기: 5×5×16 (즉, 입력 맵당 5×5, 16개 맵에 걸쳐)
- 필터 개수: 120
- 활성화 함수: tanh
- 학습 가능한 파라미터: $(5 \times 5 \times 16 + 1) \times 120 = 48120$ 파라미터



# F6 – 완전 연결 계층
  
- 입력: 이전 C5 계층의 120개 값 (1×1 특징 맵)
- 출력: 84개 뉴런
- 연결
	- 모든 120개 입력에 완전 연결
	- 각 출력 뉴런도 편향을 가짐
- 활성화 함수: tanh
- 파라미터
	- 각 출력 뉴런 → 120개 가중치 + 1개 편향
	- $(120 + 1) \times 84 = 10164$ 파라미터



## 출력 계층


- **출력**: 10개 뉴런 (숫자 분류용: 0~9)
- **주요 특징**
	- Softmax 대신 **방사 기저 함수(RBF)** 유닛 사용
	- 각 뉴런은 F6의 모든 84개 유닛에 연결됨
- **학습**
	- 역저파시 뉴런 안아 프로토타입을 학습
	- 가장 높은 활성화를 가진 출력 뉴런이 예측 클래스로 선택됨
- 절차
	- 각 RBF 뉴런은 84개의 입력을 받아 숫자의 프로토타입과 유사도를 비교한다.
	- 가장 높은 유사도를 가진 RBF 뉴런이 최종 예측 결과다.



# LeNet-5 의 특별한 설계


## RBF (출력 계층)

- 확률이 아닌 유사도 문제로 문제를 접근하기 위함
- 원래 LeNet-5 논문은 유클리드 거리 기반 RBF 유닛을 사용했습니다.
- 이들은 각 숫자 클래스의 프로토타입처럼 작동하며 입력과의 유사도를 측정합니다.


## 그룹화된 연결 (C3 계층)


- 장점
	- 매개변수 수를 줄이며 성능은 유지: 전부 연결하는 게 아니라 일부만 연결해 효율적으로 계산함.
	- 과적합 방지: 중복을 줄여서 모델이 너무 복잡해지는 것을 막음.
	- 출력 맵별 특화: 각 출력 맵이 서로 다른 입력 조합으로 특수화된 특징을 학습함.

- 그룹화된 연결

| 출력 맵 (C3)    | 사용된 입력 맵 (S2) | 출력 수 |
| ------------ | ------------- | ---- |
| 그룹 1 (0-5)   | 3개            | 6개   |
| 그룹 2 (6- 11) | 4개            | 6개   |
| 그룹 3 (12-14) | 4개            | 3개   |
| 그룹 4 (15)    | 6개            | 1개   |

- 파라미터 수
	- 총 학습 가능한 파라미터: $456 + 606 + 303 + 151 = 1516$

| 그룹   | 공식                                   | 파라미터 |
| ---- | ------------------------------------ | ---- |
| 그룹 1 | $(5 \times 5 \times 3 + 1) \times 6$ | 456  |
| 그룹 2 | $(5 \times 5 \times 4 + 1) \times 6$ | 606  |
| 그룹 3 | $(5 \times 5 \times 4 + 1) \times 3$ | 303  |
| 그룹 4 | $(5 \times 5 \times 6 + 1) \times 1$ | 151  |


