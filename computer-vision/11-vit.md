# CNN 의 귀납적 편향

**귀납적 편향**

- 모델이 본질적으로 가지고 있는 구조적 가정
- 이러한 가정들은 필요한 데이터양을 줄여주지만, 동시에 유연성을 제한함

- CNN: 의미 있는 정보들은 서로 가까이 있는 픽셀로 구성된다는 가정 (locality)
- RNN: 타임스텝 간의 순차적 의존성을 가정
- FC: 모든 입력이 동등하게 상호 작용한다는 가정


**CNN 의 귀납적 편향**

- CNN 의 강점은 강력한 귀납적 편향에서 비롯됨

1. 지역성: 의미 있는 정보는 서로 가까이 있는 픽셀들로 구성된다는 가정
	- 작은 크기의 필터를 사용해여 특정 지역에만 집중하여 구현
2. 이동 불변성: 객체의 위치가 바뀌어도 그 객체는 여전히 같은 객체
	- 가중치 공유를 사용하여 패턴이 어디에 있든 동일하게 감지

- 이러한 귀납적 편향은 CNN 이 적은 데이터로도 학습 가능하도록함
- 하지만, 더 크고 복잡한 데이터셋에서 확장성을 제안함


# Transformer 의 약한 귀납적 편향

**Transformer 는 귀납적 편향이 없습니다.**
- Transformer 는 지역성이나 이동불변성의 귀납적 편향이 없습니다.
- 대신 전역적인 Self-Attention 에 의존하여 데이터로 부터 모든 관계를 직접 학습합니다.

**이로 인한 특징**
- Data-hungry: 효과적인 학습을 위해 매우 큰 데이터셋이 필요합니다.
- 유연하고 확장 가능함: 충분한 데이터가 확보되면 전역적인 의존성을 더 효율적으로 포착 할 수 있음

---
# Vison Transformer (ViT)


**ViT 란 무엇인가**

- ViT 는 트랜스포머 아키텍쳐를 이미지에 적용한 모델

1. 이미지를 고정된 크기의 패치로 분할합니다.
2. 각 패치를 평탄화하고 선형 투형하여 임베딩 벡터로 만듭니다.
3. 분류를 위한 학습 가능한 클래스 토큰을 추가합니다.
4. 공간 정보를 보존하기 위해 포지셔널 인코딩을 추가합니다.
5. 임베딩 시퀀스들을 인코딩 레이어에 통과시킵니다.


# ViT 아키텍쳐 개요

- 입력: 패치 임베딩 + \[CLS] 토큰 + 위치 임베딩

- L개의 블록 각각 구성
	- LayerNorm
	- Muti-Head Self-Attention + 잔차 연결
	- LayerNorm
	- MLP (GELU 활성화 함수) + 잔차 연결

# ViT 패치 임베딩 과정

**동작 과정**

1. 이미지 패치 생성: 
	- 이미지를 겹치지 않는 고정된 크기의 패치로 나눕니다.
2. 패치 임베딩
	- 각 페치 (P x P x C) 를 길이가 $P^2*C$ 인 벡터로 평탄화합니다.
	- 선형 변환하여 D 차원의 임베딩 공간으로 매핑합니다.
3. Class Token 추가
	- 분류를 위한 이미지 표현 역할을 할 학습 가능한 벡터 \[CLS] 를 추가합니다.
4. 위치 임베딩 추가
	- Transformer 에는 공간 구조에 대한 정보가 없으므로, 위치 인코딩을 추가합니다.


**1. 이미지 패치 생성**

> 이미지를 크기 $P \times P$의 겹치지 않는 패치들로 분할합니다.


- 입력 이미지: $x \in \mathbb{R}^{H \times W \times C}$
- 패치: $x' \in \mathbb{R}^{N \times P \times P \times C}$  
- 총 패치 수: $N = \frac{H \times W}{P \times P}$  


**2-1. 패치 임베딩: 평탄화**

> 각 패치를 길이가 $P^2 \cdot C$ 인 벡터로 평탄화 합니다.

- 모든 패치: $x_p \in \mathbb{R}^{N \times (P^2 C)}$
- 하나의 패치: $x_p^i \in \mathbb{R}^{P^2 C}$

- CNN: 
	- 이미지의 2D 그리드 구조를 유지합니다.
- ViT: 
	- 패치를 독립적인 토큰으로 취급합니다. (NLP 의 토큰화에 가깝게 만듦)
	- 이미지의 2D 그리드 구조가 유지 되지 않습니다.
	- 하드 코딩된 지역성을 제거하고 Self-Attention 을 통해 직접 학습하도록 강제합니다.


**2-2. 선형 투영**

> 행렬 $E \in \mathbb{R}^{(P^2 C) \times D}$를 사용하여 선형 투영(FC Layer)을 적용합니다.

- 입력 (평탄화된 패치): $x_p \in \mathbb{R}^{N \times (P^2 C)}$  
- 결과 (패치 임배딩): $z \in \mathbb{R}^{N \times D}$
<br>
- ViT 의 각 패치는 동일한 Linear Layer 를 통과합니다. (모든 패치는 동일한 가중치를 공유합니다.)


**3. Class Token 추가**

**클래스 토큰**
- 클래스 토큰은 패치 임베딩 시퀀스 맨 앞에 붙는 학습 가능한 벡터 $x_{cls} \in \mathbb{R}^{D}$ 입니다.
- 추가 후 임베딩 시퀀스 크기는 $\mathbb{R}^{N \times D} \rightarrow \mathbb{R}^{(1+N) \times D}$ 로 변경됩니다.
- 이 토큰은 어떤 이미지 패치와도 대응되지 않습니다.
- 오직 분류를 위해 도입된 추가적인 파라미터입니다.

**왜 클래스 토큰을 추가하는가?**
- NLP 에서 영감을 받음
	- BERT 에서는 \[CLS] 토큰이 분류 작업을 위해 시퀀스를 요약하는 용도로 추가
	- 모델이 특별한 하나의 토큰에 전역 정보를 모으도록 함.
- 전역 정보 집계
	- CNN 은 분류전 전역 풀링을 자주 사용합니다.
	- ViT 는 미리 정의된 풀링 규칙 없이 \[CLS] 토큰을 사용하여 정보를 집계합니다.

**어떻게 동작하는가?**

- \[CLS] 토큰은 모든 패치에 주의를 기울이며 전체 이미지로부터 정보를 수집함
- Self-Attention 은 첫 레이어부터 모든 토큰이 상호작용하도록 합니다.
- 동시에, 모든 패치들도 \[CLS] 토큰에 주의를 기울일 수 있습니다.
- 이런 양방향 Attention 은 CLS 토큰이 전역 정보 허브가 될 수 있게 합니다.

**어떤 정보를 담고 있는가?**
- 모든 Transformer 인코더를 통과한 후, CLS 는 전체 이미지의 압축 표현이 됩니다.
- 이는 모든 패치에서 클래스와 관련된 패턴들을 집계한 것입니다.

**분류에서 최종 사용**
- 오직 최종 CLS 토큰만이 분류 해드 MLP 로 전달됩니다.
- 클래스 레이블을 결정하기에 충분한 전역 정보를 포함하게 됩니다.
- $y = \text{softmax}(W \cdot z_{cls})$


**4. 위치 임베딩 추가**

- 작동 방식
    1. \[CLS] 토큰을 추가한 후, 우리는 $(N+1) \times D$ 크기의 임베딩을 가집니다.
    2. 학습 가능한 위치 임베딩 행렬 $E_{pos} \in \mathbb{R}^{(N+1) \times D}$가 요소별(element-wise)로 더해집니다.
    3. 수식: $z_0 = [x_{cls}; x_p^1 E; x_p^2 E; ...; x_p^N E] + E_{pos}$  
    4. 각 위치는 자신만의 임베딩 벡터를 가지며, 이는 이미지 그리드 상의 절대 위치를 인코딩합니다.


- 위치 임베딩 종류
	1. Fixed Sinusoidal Embeddings - NLP Transformer
		- 사인과 코사인 함수를 같이 사용합니다.
		- 결정론적이며 학습되지 않습니다.
	2. Learnable Embeddings - ViT 기본값
		- 각 위치가 자신만의 학습 가능한 벡터를 가집니다.
		- 이 벡터들은 무작위로 초기화 되고 학습중에 최적화됩니다.

- 학습 가능한 임베딩
	- 단순히 패치에 번호를 매기는 방식으로 인코딩하지 않습니다.
	- 각 위치 i 는 학습 가능한 위치 임베딩 벡터 $e_i \in \mathbb{R}^D$ 를 가집니다.
	- 모델은 학습 중 위치 정보를 사용하는 최적의 방법을 배웁니다.

- 왜 학습 가능한가
	1. 이미지는 2D 이며 NLP 의 단어 순서보다 훨씬 복잡합니다.
	2. 고정된 형식은 미묘한 공간적 패턴을 포착하기에 너무 경직 될 수 있습니다.
	3. 학습 가능한 위치 임베딩은 모델이 데이터로 부터 공간적 사전 지식을 적응적으로 학습하게 해줌
	4. 경험적으로 고정 임베딩보다 더 나은 성능을 보임이 입증됨