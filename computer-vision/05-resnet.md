# 깊은 네트워크의 문제점

- 깊은 네트워크의 2가지 문제점
	1. 그라디언트 소실/폭주 문제: 이는 불안정하고 느린 훈련으로 이어짐
	2. 정확도 포화 (성능 저하 문제): 레이어를 깊게 추가하는 것이 테스트 오류 뿐만 아니라 훈련 오류도 높임

- 깊은 네트워크의 성능 저하 실험
	- 실험: 
		- CIFAR-10 데이터 셋 대상으로 20-Layer 네트워크와 56-Layer 비교
	- 결과
		- Test 에러 뿐만 아니라 Train 에러도 56-Layer 에서 나쁜 성능을 보임
	- 해석
		- 이는 과적합 문제가 아니라 최적화 실패를 나타냄
		- 추가한 네트워크가 항등 매핑 역할만 해도 20-Layer 와 동일한 성능을 내야하지만 오히려 정확도가 떨어짐
		- SGD 와 같은 최적화 알고리즘이 여러개의 비선형 레이어를 통과하면서 항등 매핑을 학습하는 걸 어려워한다는 걸 알 수 있음

- 표준 네트워크의 한계
	- 비성형 변환은 입력 신호를 조금이라도 바꾸려는 경향이 있음
	- 항등 맵핑 학습 위해 오히려 복잡하고 부자연스러운 방식으로 파라미터들을 조정하려 함
	- 이 과정에서 최적화는 길을 잃어 버림
	- 표준적인 방식으로 네트워크를 쌓았을때 항등 연산을 학습하기 너무 어렵다는 것

# 잔차 학습

- 잔차 학습의 개념
	- 표준적인 네트워크는 항등 연산 학습하기 어려움
	- 전채 매핑을 모두 학습하는 대신, 입력에서 누락된 잔차만 학습하자
	- H(x) 를 전체 함수 모두 학습하는 것 보다 입력과의 차이만 학습하는게 더 쉬움
	- 네트워크가 입력을 많이 바꿀 필요가 없다면 잔차 함수는 작은 수정만 학습하면 되기에 학습하기 쉬움

- 구성
	- 원본 매핑: H(x)
	- 재구성: H(x) = F(x) + x
	- 잔차함수: F(x) = H(x) - x
	- 스킵 연결: x

- 스킵 연결의 효과
	- 스킵 연결은 그레디언트가 방해받지 않고 흐르도록 함
	- 이 구조는 항등 맵핑을 배우기 쉬움
	- 위 2개의 이유 덕에 옵티마이저가 수렴하기 쉬워짐


- 잔차 학습의 역전파
	- 순전파 공식 일반화: $x_L = x_1 + \sum_{i=1}^{L-1} F(x_i, W_i)$  
		1. 항등 경로
			- 이 항은 모든 가중치 레이어를 우회
			- 그레디언트가 방해 받지 않고 흐름
		2. 잔차 경로
			- 이 항은 가중치를 통해 전파됨
			- 줄어들거나 폭주 가능
	- 잔차 경로가 정확히 -1 이 되는 경우는 드믈기 때문에 기울기 소실은 발생하지 않음
	- 다만 기울기 발산은 피하지 못함

# ResNet 아키텍처 구성

## ResNet 아키텍쳐 개요

- 초기 레이어
	1. 7 x 7 컨볼루션 (stride 2)
	2. 3 x 3 최대 풀링
- 4개의 잔차 블록 스테이지
	- Conv2, Conv3, Conv4, Conv5
	- 공간 해상도가 스테이지마다 절반으로 (피쳐맵 가로 세로 작아짐)
	- 채널은 각 스테이지마다 두 배로 (채널은 커짐)
- GAP
	- 특징을 1D 베터로 만듬 (채널 방향 1D, 피쳐맵 사이즈 1)
- 완전 연결 레이어 + softmax
	- 최종 분류 점수 생성

## ResNet주요 구성요소

- 잔차블록: F(x) 를 학습하고 F(x) + x 를 출력
- 스킵연결: 항등 또는 프로젝션 매핑을 통해 입력을 직접 전달
- 병목블록: 1x1 conv -> 3x3 conv ->  1x1conv (차원 축소 - 처리 - 복원)
	- 13, 18 -> 일반 블록
	- 그 이상은 병목 블록 사용 병목 블록은 더 깊은 네트워크를 가능하도록 함
-  BN + ReLU: 각 conv 레이어 이후에 적용




# ResNet 의 핵심 아이디어

## 잔차 블럭

- 특징
	- 깊은 네트워크의 성능 저하 문제를 해결하기 위해 개발
	- 매핑을 직접 학습하는 대신 입력에서 누락된 잔차만 학습
	- 표준 공식: $y = F(x, \{W_i\}) + x$  


- 장점
	1. 새로운 것에 집중
		1. 레이어는 변경이 필요한 부분 즉 항등 사상에서 입력과 차이를 학습
		2. 이는 더 깊은 네트워크에서 **최적화를 쉽게 만듬**
	2. 수렴 거동
		1. 네트워크가 잘 훈련됨에 따라 입력 x 는 출력 H(x) 에 더 가까워지므로 잔차는 0에 수렴
		2. **이는 훈련 안정화, 빠른 수렴, 필요한 최소한의 업데이트만 수행하도록 유도합니다.**
	3. 구현 단순성
		1. 아키텍쳐의 큰 변동이 필요하지 않음
		2. 단순히 숏컷만 추가하면됨
		3. x 를 재사용 하므로 추가 파라미터 필요 없음
		4. 최종 덧샘을 제외하고 추가 계산이 필요하지 않음

- 항등 매핑 종류
	1. 항등 매핑 + 패딩
		1. 채널수가 맞지 않을때 패딩 채널 추가
		2. y = F(x) + pad(x)
	2. 프로젝션 매핑
		1. 채널수가 맞지 않을때 1x1 conv 연산을 통해 채널을 확장
		2. y = F(x) + Wx
	- *ResNet 에서는 프로젝션 매핑이 사용됨: 다운샘플링이 가능하고 표현력 유지가 더 좋기 때문

## 1 X1 Convolution

- 특징
	- 높이과 너비가 1 이지만 채널 방향으로 연산
	- 아웃풋 채널 개수 만큼 사용

- 장점
	- 높이와 너비를 변경하지 않고 채널 수를 늘리거나 줄임
	- 비용이 많이드는 conv 연산 전에 파라미터와 연산수를 줄임
	- 활성화 함수가 뒤따르면 비선형성이 추가됨 (축소와 연산 과정에만 Relu 따라옴)
	- 병복 블럭에 사용 (채널 압축 conv 확장)


## 배치 정규화

- 내용이 길어서 새로운 포스팅으로 분리했습니다.
- 


## 사전 활성화

- 일반 ResNet 잔차 블록 내부 순서: Conv -> BN -> ReLU - Conv -> BN -> Addition -> ReLU
- 사잔활서와 잔차 블록 내부 순서: BN -> ReLU -> Conv
- 이 방식은 부드러운 최적화와 더 나은 그래디언트 흐름을 제공

