

# AlexNet 소개


**동기**
 - MNIST 와 같은 간단한 작업은 얕은 네트워크도 충분
 - ImageNet 과 같이 복잡한 작업은 깊은 네트워크가 필요
- 모델은 특징을 계층적으로 학습해야함: 모서리, 엣지 -> 패턴, 질감 -> 부품, 의미론
- 네트워크가 갚어질수록 더 추상적인 특징을 포착할 수 있음 
- 네트워크가 깊어질수록 최고 계층은 더 고수준의 이해로 결정을 내림

**아키텍쳐**
- 5개의 conv 와 3개의 FC 계층
- tanh 대신 ReLU 활성화 함수
- 드롭아웃 기법 사용
- 병렬 GPU사용


**중요기술**
- ReLU
- GPU 병렬 사용
- 데이터 증강
- 드롭아웃
- LRN (로컬 응답 정규화)

역사적 의의
- CNN 이 수공 특징을 이긴다는 걸 증명
- 데이터와 컴퓨팅 파워로 무궁한 딥러닝 잠재력을 보여줌



# AlexNet 아키텍쳐

## AlexNet 아키텍쳐 개요

- 훈련 가능한 파라미터를 가진 8개 계층: 5개 합성곱 계층과 3개 완전 연결 계층
- 입력: RGB 이미지, 227 × 227 × 3 채널 (빨강, 파랑, 초록)
- 출력: 1000-class 소프트맥스
- 계산을 분할하기 위해 두 GPU 사용

## 계층1 — 합성곱 + ReLU + Pooling + LRN

- 필터(커널): 11×11×3 크기의 96개 필터 → 96개 출력 채널
- 스트라이드 = 4, 패딩 없음
- 출력 크기: 55×55×96 채널
- 활성화 함수: ReLU 활성화
- 중첩 최대 풀링: 3×3 윈도우, 스트라이드 2
- 로컬 응답 정규화 (LRN)


## 계층 2 — 병렬 합성곱 + ReLU + Pooling + LRN

- GPU 2개 병렬 처리가 특징

- 입력
	- 55 × 55 × 96
- 병렬 합성곱
	- 5×5×48 크기의 128개 필터 (2개 GPU로 인해 2세트)
	- 스트라이드 = 1, 패딩 = 2
	- 출력: 27 × 27 × 256 (128 × 2 GPUs)

- 중첩 최대 풀링
	- 3×3, 스트라이드 2 → 13×13×256
- LRN 다시 적용

## 계층 3 to 5 — 병렬 합성곱 + ReLU + 풀링 + LRN

- 추상적이고 복잡한 특징을 학습하기 위해 사용

- 계층 3 (Layer 3)
    - 3×3×256 크기의 384개 필터
    - 2개의 GPU가 192개 채널(필터)**씩 나누어 병렬 처리
    - 활성화 함수: ReLU
    - 전체 출력: 13×13×384
        
- 계층 4 (Layer 4)
    - 3×3×192 크기의 384개 필터
    - 2개의 GPU가 192개 채널(필터)**씩 나누어 처리
    - 이때 GPU 간 일부 피처맵을 교환
    - 활성화 함수: ReLU
    - 전체 출력: 13×13×384
        
- 계층 5 (Layer 5)
    - 3×3×192 크기의 256개 필터
    - 2개의 GPU가 128개 채널(필터)**씩 나누어 처리
    - 일부 피처맵 교환 가능
    - 활성화 함수: ReLU
    - 출력 후 Max Pooling: 6×6×256 = 9216


## 계층 6 to 8 — 완전 연결 + Classification

- 계층 6
	- FC(9216 → 4096), ReLU, 드롭아웃
- 계층 7
	- FC(4096 → 4096), ReLU, 드롭아웃
- 계층 8
	- FC(4096 → 1000), 소프트맥스




# AlexNet 의 핵심 기술들

## ReLU

- 수식: ReLU(x) = max(0, x)
- 그래프: 음수 입력 → 0 / 양수 입력 → 선형 증가

- ReLU 장점
	- ReLU 는 단순해서 개선 효율성이 높음
	-  기울기 소실 장지 덕에 빠른 수렴


| 속성       | Sigmoid / tanh (포화) | ReLU (비포화) |
| -------- | ------------------- | ---------- |
| 출력 범위    | [0,1] 또는 [-1,1]     | [0, ∞)     |
| 0 근처 기울기 | 매우 작음               | 1          |
| 계산       | 지수 함수 계산            | 단순 최대 연산   |
| 수렴 속도    | 느림                  | 빠름         |



## LRN

- 동일한 공간 위치에서 채널 간에 적용
- ReLU 이후 극단적인 큰 값이 나타나고 이는 학습을 지배할 수 있음
- 극단적인 활성화 값 억제 각 뉴런간 경쟁을 촉진함
- 각 필터가 고유하고 전문적인 특징을 학습할 수 있도록
- AlexNet에서 Conv1과 Conv2에서만 사용



- 공식
	- $a_{x,y}^i$: i번째 채널의 위치(x, y)에서 ReLU 출력
	- N: 특징 맵의 총수
	- n: 로컬 윈도우 크기 (일반적으로 5)
	- $k = 2, α = 10^{-4}, β = 0.75$ (AlexNet 논문에서 사용)

$$
b_{x,y}^i = a_{x,y}^i / (k + α * Σ_{j=max(0,i-n/2)}^{min(N-1,i+n/2)} (a_{x,y}^j)^2)^β
$$




## 중첩 맥스 풀링

- 중첩풀링이란?
	- 커널 사이즈보다 작은 스트라이드 값을 사용 (kernel=3x3, stride=2)


- 중첩 풀링 특징
	- 초기 계층에서 더 많은 공간 세부 사항을 유지하는 데 유용
	- 입력이 작고 정보 손실이 중요할 때 도움이 됨
	- 비중첩 풀링보다 더 많은 계산 필요
	- 오늘날에는 덜 일반적으로 사용됨 
	- (종종 스트라이드 합성곱이나 어텐션과 같은 다른 기술로 대체됨)


- 비중첩 풀링 vs 중첩 풀링

| 지표               | 비중첩 (Stride = 2) | 중첩 (Stride = 1) |
| ---------------- | ---------------- | --------------- |
| 정보               | 더 많은 손실          | 더 적은 손실         |
| 수용 필드            | 드문드문             | 빽뺵              |
| 정확도 (AlexNet 논문) | 더 낮음             | 더 높음            |
| 수렴 속도            | 더 느림             | 더 빠름            |

> 수용 필드: 출력값 하나를 만들기 위해 실제로 '참고하는 입력 구간'


